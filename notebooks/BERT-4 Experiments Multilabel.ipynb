{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel BERT Experiments\n",
    "\n",
    "In this notebook we do some first experiments with BERT: we finetune a BERT model+classifier on each of our datasets separately and compute the accuracy of the resulting classifier on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments we use the `pytorch_transformers` package. It contains a variety of neural network architectures for transfer learning and pretrained models, including BERT and XLNET.\n",
    "\n",
    "Two different BERT models are relevant for our experiments: \n",
    "\n",
    "- BERT-base-uncased: a relatively small BERT model that should already give reasonable results,\n",
    "- BERT-large-uncased: a larger model for real state-of-the-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multilabel import EATINGMEAT_BECAUSE_MAP, EATINGMEAT_BUT_MAP, JUNKFOOD_BECAUSE_MAP, JUNKFOOD_BUT_MAP\n",
    "\n",
    "label_map = EATINGMEAT_BECAUSE_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.modeling_bert import BertForSequenceClassification\n",
    "\n",
    "BERT_MODEL = 'bert-large-uncased'\n",
    "BATCH_SIZE = 16 if \"base\" in BERT_MODEL else 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 if \"base\" in BERT_MODEL else 8\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use the same data as for all our previous experiments. Here we load the training, development and test data for a particular prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Meat industry produces greenhouse gases and/or uses water - specific numbers': 570, 'Meat industry produces greenhouse gases and/or uses water - general': 548, 'Because as preposition': 166, 'Meat industry harms environment/uses resources w/o mentioning greenhouse gases or water': 70, \"Outside of article's scope\": 30, 'Meat industry produces greenhouse gases and/or uses water - incorrect numbers or comparison': 12, 'Irrelevant fact from article': 10, 'Meat industry harms animals': 5})\n",
      "4456\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "prefix = \"eatingmeat_because_xl\"\n",
    "train_file = f\"../data/interim/{prefix}_train_withprompt.ndjson\"\n",
    "synth_files = glob.glob(f\"../data/interim/{prefix}_train_withprompt_allsynth.ndjson\")\n",
    "dev_file = f\"../data/interim/{prefix}_dev_withprompt.ndjson\"\n",
    "test_file = f\"../data/interim/{prefix}_test_withprompt.ndjson\"\n",
    "\n",
    "with open(train_file) as i:\n",
    "    train_data = ndjson.load(i)\n",
    "\n",
    "synth_data = []\n",
    "for f in synth_files:\n",
    "    with open(f) as i:\n",
    "        synth_data += ndjson.load(i)\n",
    "    \n",
    "with open(dev_file) as i:\n",
    "    dev_data = ndjson.load(i)\n",
    "    \n",
    "with open(test_file) as i:\n",
    "    test_data = ndjson.load(i)\n",
    "    \n",
    "labels = Counter([item[\"label\"] for item in train_data])\n",
    "print(labels)\n",
    "print(len(synth_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build the label vocabulary, which maps every label in the training data to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'harms environment': 0, 'irrelevant': 1, 'harms animals': 2, 'because as preposition': 3, 'greenhouse gas or water': 4, 'specific numbers': 5, 'outside scope': 6, 'incorrect numbers': 7}\n",
      "{0: 'harms environment', 1: 'irrelevant', 2: 'harms animals', 3: 'because as preposition', 4: 'greenhouse gas or water', 5: 'specific numbers', 6: 'outside scope', 7: 'incorrect numbers'}\n"
     ]
    }
   ],
   "source": [
    "label2idx = {}\n",
    "idx2label = {}\n",
    "target_names = []\n",
    "for item in label_map:\n",
    "    for label in label_map[item]:\n",
    "        if label not in target_names:\n",
    "            idx = len(target_names)\n",
    "            target_names.append(label)\n",
    "            label2idx[label] = idx\n",
    "            idx2label[idx] = label\n",
    "    \n",
    "print(label2idx)\n",
    "print(idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_multilabel(items):\n",
    "    return [{\"text\": item[\"text\"], \"label\": label_map[item[\"label\"]]} for item in items]\n",
    "\n",
    "train_data = map_to_multilabel(train_data)\n",
    "dev_data = map_to_multilabel(dev_data)\n",
    "synth_data = map_to_multilabel(synth_data)\n",
    "test_data = map_to_multilabel(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1411\n",
      "Train data size: 5867\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample(train_data, synth_data, label2idx, number):\n",
    "    \"\"\"Sample a fixed number of items from every label from\n",
    "    the training data and test data.\n",
    "    \"\"\"\n",
    "    new_train_data = []\n",
    "    for label in label2idx:\n",
    "        data_for_label = [i for i in train_data if i[\"label\"] == label]\n",
    "        \n",
    "        # If there is more training data than the required number,\n",
    "        # take a random sample of n examples from the training data.\n",
    "        if len(data_for_label) >= number:\n",
    "            random.shuffle(data_for_label)\n",
    "            new_train_data += data_for_label[:number]\n",
    "            \n",
    "        # If there is less training data than the required number,\n",
    "        # combine training data with synthetic data.\n",
    "        elif len(data_for_label) < number:\n",
    "            \n",
    "            # Automatically add all training data\n",
    "            new_train_data += data_for_label\n",
    "            \n",
    "            # Compute the required number of additional data\n",
    "            rest = number-len(data_for_label)\n",
    "            \n",
    "            # Collect the synthetic data for the label\n",
    "            synth_data_for_label = [i for i in synth_data if i[\"label\"] == label]\n",
    "            \n",
    "            # If there is more synthetic data than required, \n",
    "            # take a random sample from the synthetic data.\n",
    "            if len(synth_data_for_label) > rest:\n",
    "                random.shuffle(synth_data_for_label)\n",
    "                new_train_data += synth_data_for_label[:rest]\n",
    "            # If there is less synthetic data than required,\n",
    "            # sample with replacement from this data until we have\n",
    "            # the required number.\n",
    "            else:\n",
    "                new_train_data += random.choices(synth_data_for_label, k=rest)\n",
    "        \n",
    "    return new_train_data\n",
    "\n",
    "\n",
    "def random_sample(train_data, train_size):\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[:TRAIN_SIZE]    \n",
    "\n",
    "#train_data = sample(train_data, synth_data, label2idx, 200)\n",
    "print(\"Train data size:\", len(train_data))\n",
    "train_data = train_data + synth_data\n",
    "print(\"Train data size:\", len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We load the pretrained model and put it on a GPU if one is available. We also put the model in \"training\" mode, so that we can correctly update its internal parameters on the basis of our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
    "\n",
    "\n",
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained(BERT_MODEL, num_labels=len(label2idx))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We preprocess the data by turning every example to an `InputFeatures` item. This item has all the attributes we need for finetuning BERT: \n",
    "\n",
    "- input ids: the ids of the tokens in the text\n",
    "- input mask: tells BERT what part of the input it should not look at (such as padding tokens)\n",
    "- segment ids: tells BERT what segment every token belongs to. BERT can take two different segments as input\n",
    "- label id: the id of this item's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/27/2019 15:45:26 - INFO - __main__ -   *** Example ***\n",
      "08/27/2019 15:45:26 - INFO - __main__ -   text: Large amounts of meat consumption are harming the environment, because it creates one-fifth of the earth's greenhouse gases.\n",
      "08/27/2019 15:45:26 - INFO - __main__ -   input_ids: 101 2312 8310 1997 6240 8381 2024 7386 2075 1996 4044 1010 2138 2009 9005 2028 1011 3587 1997 1996 3011 1005 1055 16635 15865 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/27/2019 15:45:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/27/2019 15:45:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/27/2019 15:45:26 - INFO - __main__ -   label:['harms environment', 'greenhouse gas or water', 'specific numbers'] id: [1. 0. 0. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_SEQ_LENGTH=100\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        \n",
    "\n",
    "def convert_examples_to_features(examples, label2idx, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    features = []\n",
    "    for (ex_index, ex) in enumerate(examples):\n",
    "        \n",
    "        # TODO: should deal better with sentences > max tok length\n",
    "        input_ids = tokenizer.encode(\"[CLS] \" + ex[\"text\"] + \" [SEP]\")\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "            \n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_ids = np.zeros(len(label2idx))\n",
    "        for label in ex[\"label\"]:\n",
    "            label_ids[label2idx[label]] = 1\n",
    "        \n",
    "        if verbose and ex_index == 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"text: %s\" % ex[\"text\"])\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label:\" + str(ex[\"label\"]) + \" id: \" + str(label_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=label_ids))\n",
    "    return features\n",
    "\n",
    "train_features = convert_examples_to_features(train_data, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "dev_features = convert_examples_to_features(dev_data, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(test_data, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize data loaders for each of our data sets. These data loaders present the data for training (for example, by grouping them into batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.float)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE)\n",
    "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE)\n",
    "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation method takes a pretrained model and a dataloader. It has the model predict the labels for the items in the data loader, and returns the loss, the correct labels, and the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sigmoid\n",
    "\n",
    "def evaluate(model, dataloader, verbose=False):\n",
    "\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        sig = Sigmoid()\n",
    "        outputs = sig(logits).to('cpu').numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs >= 0.5)        \n",
    "        correct_labels += list(label_ids)\n",
    "                    \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let's prepare the training. We set the training parameters and choose an optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "\n",
    "num_train_steps = int(len(train_data) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=100, t_total=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the actual training. In each epoch, we present the model with all training data and compute the loss on the training set and the development set. We save the model whenever the development loss improves. We end training when we haven't seen an improvement of the development loss for a specific number of epochs (the patience). \n",
    "\n",
    "Optionally, we use gradient accumulation to accumulate the gradient for several training steps. This is useful when we want to use a larger batch size than our current GPU allows us to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd594efd450446c09c643034caa6bd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd92e22760174f67aa758b21e52859a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: []\n",
      "Dev loss: 0.12213541412105162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|         | 1/20 [05:45<1:49:32, 345.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c2526530942d0804bf51c454eb976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa9f963f6b24d759b62610066334224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162]\n",
      "Dev loss: 0.0877677650617746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|         | 2/20 [11:32<1:43:48, 346.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4783bc96a4942dd9a7721cff28072bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7fd17127e94650a52b2e16825b84e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746]\n",
      "Dev loss: 0.06956643640296534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|        | 3/20 [17:18<1:38:01, 345.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4368f17cb9894e96ad0bfb3d9f769bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b847503eb1504035b729d2bb7f321273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|        | 4/20 [23:02<1:32:10, 345.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746, 0.06956643640296534]\n",
      "Dev loss: 0.07502751060140629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5bc42decfa415d9cbdf3331de4eb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5688a6153754c88a591c58113cc1c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|       | 5/20 [28:47<1:26:20, 345.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746, 0.06956643640296534, 0.07502751060140629]\n",
      "Dev loss: 0.08636674445006065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d64be0b166d4ee68f1b3ec1a2cae31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f104d6cbb14023bd344231f0d851e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|       | 6/20 [34:32<1:20:33, 345.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746, 0.06956643640296534, 0.07502751060140629, 0.08636674445006065]\n",
      "Dev loss: 0.09042651116033085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff30f4d4a045467cac2cd220ad7cb55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abed55ec7f9492b9a0bf65092a96a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|      | 7/20 [40:17<1:14:45, 345.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746, 0.06956643640296534, 0.07502751060140629, 0.08636674445006065, 0.09042651116033085]\n",
      "Dev loss: 0.10135477319515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f36b89321ec40029d9060e80213832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=2934, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b7f6f578a844a5b8b485a9f8dbfde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=48, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [0.12213541412105162, 0.0877677650617746, 0.06956643640296534, 0.07502751060140629, 0.08636674445006065, 0.09042651116033085, 0.10135477319515]\n",
      "Dev loss: 0.09101424391944117\n",
      "No improvement on development set. Finish training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "OUTPUT_DIR = \"/tmp/\"\n",
    "MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "PATIENCE = 5\n",
    "\n",
    "global_step = 0\n",
    "model.train()\n",
    "loss_history = []\n",
    "best_epoch = 0\n",
    "for epoch in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        outputs = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            lr_this_step = LEARNING_RATE * warmup_linear(global_step/num_train_steps, WARMUP_PROPORTION)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "    dev_loss, _, _ = evaluate(model, dev_dataloader)\n",
    "    \n",
    "    print(\"Loss history:\", loss_history)\n",
    "    print(\"Dev loss:\", dev_loss)\n",
    "    \n",
    "    if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    if epoch-best_epoch >= PATIENCE: \n",
    "        print(\"No improvement on development set. Finish training.\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    loss_history.append(dev_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We load the pretrained model, set it to evaluation mode and compute its performance on the training, development and test set. We print out an evaluation report for the test set.\n",
    "\n",
    "Note that different runs will give slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /tmp/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/27/2019 16:31:30 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/yves/.cache/torch/pytorch_transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "08/27/2019 16:31:30 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 8,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/27/2019 16:31:31 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/yves/.cache/torch/pytorch_transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f64b4d899d44dd7aadbde34e941d6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=72, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "output_model_file = \"/tmp/pytorch_model.bin\"\n",
    "print(\"Loading model from\", output_model_file)\n",
    "device=\"cpu\"\n",
    "\n",
    "model_state_dict = torch.load(output_model_file, map_location=lambda storage, loc: storage)\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels=len(label2idx))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "_, test_correct, test_predicted = evaluate(model, test_dataloader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.979933110367893\n",
      "R: 0.9606557377049181\n",
      "A: 0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "all_correct = 0\n",
    "fp, fn, tp, tn = 0, 0, 0, 0\n",
    "for c, p in zip(test_correct, test_predicted):\n",
    "    if sum(c == p) == len(c):\n",
    "        all_correct +=1\n",
    "    for ci, pi in zip(c, p):\n",
    "        if pi == 1 and ci == 1:\n",
    "            tp += 1\n",
    "            same = 1\n",
    "        elif pi == 1 and ci == 0:\n",
    "            fp += 1\n",
    "        elif pi == 0 and ci == 1:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "            same =1\n",
    "            \n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"P:\", precision)\n",
    "print(\"R:\", recall)\n",
    "print(\"A:\", all_correct/len(test_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large amounts of meat consumption are harming the environment, because it creates one-fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it affects the environment.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because it creates greenhouse gases and uses a lot of water resources.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it is causing a very excessive amount of greenhouse gasses.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of greenhouse gases from transportation and animals themselves.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because of the amount of water used to raise livestock and the carbon impact of transporting meat products.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because production of meat produces huge amounts of greenhouse gasses#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because this process is responsible for almost one-fifth of the earth's greenhouse gases which is harmful for the environment.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transporting meat releases massive amounts of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because natural resources are used raising and transporting products.#harms environment;greenhouse gas or water#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because raising the livestock and transporting it accounts for almost 1/5 of the earth's greenhouse gases#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising livestock produces more greenhouse gas.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it is hurting the earth by contributing to one-fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it increases the amount of greenhouse gases present in the atmosphere#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting meat creates more greenhouse gases than driving most vehicles combined.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gases it produces, and water it uses.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because it takes a lot of energy, resources, and effort to sustain the meat industry.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting livestock is causing a large portion of the greenhouse gases produced.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it hurts the economy and takes away jobs.#irrelevant#irrelevant\n",
      "Large amounts of meat consumption are harming the environment, because greenhouse gases#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it produces a large amount of greenhouse gas and water.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of the transportation and upkeep of the livestock we depend on for the meat.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because transportation of animals causes pollution.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting livestock causes 1/5 of all green house gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because according to the article they create a lot of greenhouse gases#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it requires a lot of resources contributing to 20% of green house gases generated by man.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the meat industry produces 1/5 of the green house gasses.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it is a large contributor to greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it does not say#outside scope#\n",
      "Large amounts of meat consumption are harming the environment, because of the enormous amount of greenhouse gases produced by raising and transporting cattles#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because raising animals for food cause water pollution and animals suffering.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because they produce almost one-fifth of greenhouse gases, as well as consuming almost half of all water used in the US.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it creates one fifth of earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because greenhouses gases#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because a large portion of greenhouse gasses are caused by raising and transporting meat.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the more we consume, the higher amounts of raising and transporting animal meat creates more greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the production of meats create greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because transporting the meat causes the use of a lot of energy and pollution.#harms environment#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because we kill too many animals#outside scope#outside scope\n",
      "Large amounts of meat consumption are harming the environment, because of the increased amounts of greenhouse gases that result from it.#because as preposition#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting animals accounts for 20% of greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because meat production creates nearly twenty-five percent of greenhouse gases in our planet's atmosphere.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gas emissions that come from the meat industry.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because it produces about one fifth of the greenhouse gases in the earth.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it is creating a lot of greenhouse gas and using up water.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because they contribute to higher greenhouse gases and water consumption.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting meat causes greenhouse gases and too much water is used.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gases produced by the raising of animals and transporting the meat.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because the meat industry produces 1/5th of all greenhouse gases on Earth.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transporting meat produces high amounts of greenhouse gasses.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of what is required to produce enough for everyone.#because as preposition#\n",
      "Large amounts of meat consumption are harming the environment, because continued demand puts an enormous strain on the environment, roughly 20% of greenhouse gasses produced comes from the meat industry#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because Half of the US water consumption is used for raising livestock.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting livestock creates a massive amount of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gases produced and quantity of water consumed in the production of meat.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because they emit more than one fifth of overall greenhouse gases#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the release of greenhouse gasses.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the raising and transportation of the animals creates greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of green house effect is increasing!#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because meat production creates huge amounts of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because they generate one fifth of greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because exporting and greenhouse gases increase as a result.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising the livestock and transporting meat produces one-fifth of the earth's greenhouse gases, which is more than that produced by cars, trucks, trains, and airplanes combined.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it produces more green house gases from the delivery trucks.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transporting meats cause almost one-fifth of greenhouse gases on earth.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because almost one fifth of the world's greenhouse gases are created by raising animals and transporting meat.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting animals produces 1/5 of earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the meat industry produces one-fifth of the greenhouse gases on earth, more than cars, trucks, trains, and planes combined.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it contributes to the creation of 1/5 of the Earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because more domestic animals that are suppose to complete the Eco circle are killed every day.#harms animals#outside scope\n",
      "Large amounts of meat consumption are harming the environment, because they account for 1/5 of greenhouse gasses#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because one-fifth of the earths greenhouse gases are produced by raising and transporting meat#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it is responsible for one fifths of the greenhouse gas emissions.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it produces more greenhouse gases than cars, planes, and trains all together; one fifth in total.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because almost one third of the earths greenhouse gases come from raising animals and transporting them#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of the thought that greenhouse gas is emitted from raising meat and a large amount of water is consumed.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because the process involved with bringing meat to grocery stores takes up quite a large portion of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it creates the earth's greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because they contribute significantly to greenhouse gas production.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because transporting meat produces 1/5 of the earth's greenhouse gases and almost 1/2 of water usage in the US is dedicated to raising livestock.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of gases being relased#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting meat is responsible for creating around one-fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because greenhouse gasses produced and environmental impact.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising cattle and shipping out the meat causes more gas in the environment than cars, trucks and planes put together.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because they produce far too many green house gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it increases greenhouse gases more than cars, trucks, trains, and airplanes combined.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising all the animals and transporting meat creates at least one-fifth of the earths greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because transport of this creates 1/5 of Earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it generates greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because they increase the amount of greenhouse emissions.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it is producing  one-fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of the large amount of water used  and greenhouse gases emitted by livestock.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because raising livestock and transporting meat produces almost one-fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the transport and production stages of meat cause a massive amount of damage to the environment.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting meat generates almost 1/5 of the world's greenhouse gasses.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because one-fifth of the earth's greenhouse gases are produced by the meat industry.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it creates a large amount of greenhouse gases, and also uses a large amount of water to raise the animals.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising livestock for consumption creates 1/5th of the earth's greenhouse gases, and uses large amounts of water.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because they are putting out more  green house gasses that is coming from all the gasses from shipping these animals for people to eat.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gas emissions and water needs involved in raising livestock.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transporting animal meat results in about one-fifth of harmful greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it produces gases#harms environment;greenhouse gas or water#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because of the greenhouse gases that are produced transporting the meat.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because of the amount of greenhouse gasses  associated with production and transportation of meat.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because the raising and transporting of meat is a large contributor  of green house gases#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the cost of transporting these meat products.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because of the green house gases it causes when raising the animal#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because they are creating an excess of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because Most of the beef produced domestically is eaten by Americans#irrelevant#\n",
      "Large amounts of meat consumption are harming the environment, because Growing cow to selling butchered meat now we are exporting our meats is bad carbon footprint.#harms environment#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because it creates increased greenhouse gases#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting live-stock adds large amounts of greenhouse gasses to our planet's atmosphere.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the greenhouse gases caused by raising and transporting animals that will be consumed as meat are greater than that of all cars, trucks, trains, and airplanes combined.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because raising and transporting animals creates about 1/5 of the earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of greenhouse gases.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because half the water in the US is used to raise livestock.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it creates one fifth of earths greenhouse gases#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the raising and transporting of meat creates one fifth of earths greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because one-fifth of the earths greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of greenhouse gases that produced by raising livestock as well as transporting animals in vehicles.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because many greenhouse gasses are emitted in the raising and transporting of livestock for the meat market.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the people kill a lot of animal to satisfy the people desire.#outside scope#outside scope\n",
      "Large amounts of meat consumption are harming the environment, because of greenhouse gases produced.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because the production of meat causes greenhouse gasses.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because the rate at which we farm domestic animals creates one-fifth of Earth's greenhouse gasses, and that industry is only expanding.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because they comprise one-fifth of the Earth's greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transporting them account for 20% of greenhouse gases on earth.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it is creating greenhouse gases and uses up a lot of water.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because this creates almost one fifth of the earth's greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the transportion of meat creates large amounts of greenhouse gases.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because of greenhouse gases produced by cars, trucks, trains, and airplanes combined.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because the cost of transporting the meats causes 1/5 of greenhouse gasses.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because hurting the economy and taking away precious jobs.#irrelevant#irrelevant\n",
      "Large amounts of meat consumption are harming the environment, because between raising livestock and transporting said livestock about one fifth of all greenhouse gasses are created as well as over half of the water in america is going to livestock.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because of carbon emissions.#because as preposition#because as preposition\n",
      "Large amounts of meat consumption are harming the environment, because raising animals for food creates large amounts of greenhouses gasses that contribute to global warming and half of the water we use is used on raising livestock for food.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because meat production releases emissions into the atmosphere and uses precious water resources.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because processing meat uses resources and creates pollution.#harms environment#harms environment\n",
      "Large amounts of meat consumption are harming the environment, because raising animals and transportation of meat uses a significant amount of the earth's greenhouses gases, while the raising of livestock also uses half of the water used in the U.S.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because transporting meat and raising the animals  creates almost one-fifth of greenhouse gases.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because it takes a lot of resources such as water and land to raise livestock.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because THEY PRODUCE 1/5 THE GREENHOUSE GASES#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water;specific numbers\n",
      "Large amounts of meat consumption are harming the environment, because the meat industry generates insane amounts of greenhouse gases and consumes almost half of America's water usage.#harms environment;greenhouse gas or water;specific numbers#harms environment;greenhouse gas or water\n",
      "Large amounts of meat consumption are harming the environment, because most of our water sources are used for the animals we are raising for meat.#harms environment;greenhouse gas or water#harms environment;greenhouse gas or water\n"
     ]
    }
   ],
   "source": [
    "for item, predicted, correct in zip(test_data, test_predicted, test_correct):\n",
    "    correct_labels = [idx2label[i] for i, l in enumerate(correct) if l == 1]\n",
    "    predicted_labels = [idx2label[i] for i, l in enumerate(predicted) if l == 1]\n",
    "    print(\"{}#{}#{}\".format(item[\"text\"], \";\".join(correct_labels), \";\".join(predicted_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
