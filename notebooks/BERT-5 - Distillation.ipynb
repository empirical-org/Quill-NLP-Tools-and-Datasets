{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: masking\n",
      "Iteration 0: n-gram sampling\n",
      "Iteration 1: masking\n",
      "Iteration 1: n-gram sampling\n",
      "Iteration 2: masking\n",
      "Iteration 2: n-gram sampling\n",
      "Iteration 3: masking\n",
      "Iteration 3: n-gram sampling\n",
      "Iteration 4: masking\n",
      "Iteration 4: n-gram sampling\n",
      "Iteration 5: masking\n",
      "Iteration 5: n-gram sampling\n",
      "Iteration 6: masking\n",
      "Iteration 6: n-gram sampling\n",
      "Iteration 7: masking\n",
      "Iteration 7: n-gram sampling\n",
      "Iteration 8: masking\n",
      "Iteration 8: n-gram sampling\n",
      "Iteration 9: masking\n",
      "Iteration 9: n-gram sampling\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "\n",
    "from quillnlp.distillation.augmentation import augment_data\n",
    "\n",
    "PREFIX = \"eatingmeat_but_xl\"\n",
    "\n",
    "if \"meat\" in PREFIX:\n",
    "    PROMPT = \"Large amounts of meat consumption are harming the environment, \"\n",
    "elif \"junkfood\" in PREFIX:\n",
    "    PROMPT = \"Schools should not allow junk food to be sold on campus \"\n",
    "\n",
    "train_file = f\"../data/interim/{PREFIX}_train_withprompt.ndjson\"\n",
    "dev_file = f\"../data/interim/{PREFIX}_dev_withprompt.ndjson\"\n",
    "test_file = f\"../data/interim/{PREFIX}_test_withprompt.ndjson\"\n",
    "\n",
    "with open(train_file) as i:\n",
    "    train_data = ndjson.load(i)\n",
    "    \n",
    "with open(dev_file) as i:\n",
    "    dev_data = ndjson.load(i)\n",
    "    \n",
    "with open(test_file) as i:\n",
    "    test_data = ndjson.load(i)\n",
    "    \n",
    "augmented_data = augment_data([x[\"text\"] for x in train_data], prompt=PROMPT, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16660"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of the American',\n",
       " 'Large amounts of meat consumption are harming the environment, but harming [MASK] [MASK] jobs!',\n",
       " 'Large amounts of meat consumption are harming the environment, but it could hurt American businesses and jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] meat production will [MASK] us economically because it is one of the major American domestic industries.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] it is significant to consume meat in moderation.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat industry is rising  reducing [MASK]could damage [MASK] important position that the infrastructure taking industry  economy but loss of precious jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat consumption [MASK] harm [MASK] economy and take away [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, but helps the economy by creating lucrative jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat industry is a powerful world economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat consumption can also harm an economy.',\n",
       " \"Large amounts of meat consumption are harming the environment, but they are [MASK] important but a symbolic part of many American's outcomes\",\n",
       " 'Large amounts of meat consumption are harming the environment, but getting rid of the meat industry would altogether hurt the part.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] everyone eats meat, so itwould basically impossible up heavily stop nutrition production.',\n",
       " 'balanced ,',\n",
       " 'Large amounts of meat consumption are harming the environment, but meat is [MASK] delicious to give up.',\n",
       " 'Large amounts of meat consumption are harming the environment, but it are a part of our culture and they employ a lot of people.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] good [MASK] the [MASK] for the economy',\n",
       " 'domestic industry is thriving',\n",
       " 'but people like the',\n",
       " 'Large amounts of meat consumption are harming the environment, but cutting back on meat jobs could be Large to the US economy to exports are [MASK] [MASK] and the industry is [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is contributing to [MASK] in Americans and away of our culture.',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is also [MASK] the economy and providing many jobs.',\n",
       " 'needs mostly',\n",
       " 'Large amounts of meat consumption are harming the environment, but meat production is the leading [MASK] of greenhouse gasses [MASK] adding to [MASK] change.',\n",
       " 'Large amounts of meat consumption are harming the environment, but helping the economy and creating [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, but exports are increasing as countries becoming meat consumption.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] for many [MASK] ithas [MASK] of the tradition.',\n",
       " 'Large amounts of meat consumption are harming the environment, but less [MASK] would cause a loss of meat with many.',\n",
       " 'Large amounts of meat consumption are harming the environment, but I do not like vegetables [MASK] [MASK] will continue to eat meat.',\n",
       " 'Large amounts of meat consumption are harming the environment, but economy is ingrained in [MASK] culture.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing [MASK] consumption would [MASK] the meat and is part.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the loss of the valuable industry and associated jobs would harm our economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but removing the people from the markets [MASK] harm economies.',\n",
       " 'Large amounts of meat consumption are harming the environment, but that are not mean that meat stopped to be eliminated from which American diet because those who [MASK] a [MASK] ideology eat a mainly environment based diet that is supplemented of meat.',\n",
       " 'Large amounts of meat consumption are harming the environment, but people still tend to eat we due to holidays [MASK] it live the part of their people.',\n",
       " 'Large amounts of meat consumption are harming the environment, but there are ways to [MASK] it.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat [MASK] is thriving due to meat consumption, and the [MASK] also provides jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, and it [MASK] harming our jobs to make [MASK] change.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] [MASK] creating jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing it could hurt the economy through the loss of jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but eat all',\n",
       " 'Large amounts of meat consumption are harming the environment, but due to meat[MASK] role of American [MASK] and consumption, eliminating meat altogether is a dangerous proposition for [MASK] American meat-of-life.',\n",
       " 'e.g. , transportation , sales',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] helping on the individual on the environment',\n",
       " 'Large amounts of meat consumption are harming the environment, but people who eat [MASK] are healthier and happier.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat and transportation provides many jobs but [MASK] sustain [MASK] economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] meat industry is enjoy of to exports increasing as more countries integrate meat into their [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, but it are some meat in our [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] some people believe that [MASK] moderate amount [MASK] meat consumption is [MASK] [MASK] compromise.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] [MASK] [MASK] consumption harming harm an industry that is are worldwide.',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is imported to other countries as away as consumed if the USA, which has made it an important domestic industry.',\n",
       " 'Large amounts of meat consumption are harming the environment, but we need [MASK] have the meat to maintain good and healthy body',\n",
       " 'Large amounts of meat consumption are harming the environment, but some people are choosing to be \"flexitarians\" and eating less meat for the meals.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat consumption could cause a tradition [MASK] jobs and is the economy',\n",
       " 'Large amounts of meat consumption are harming the environment, but meat consumption is so ingrained within American culture [MASK] many would find it very difficult to cut back.',\n",
       " 'Large amounts of meat consumption are harming the environment, but lowering the consumption of meat would harm the production and cost amounts.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] almost half the water used in the [MASK] goes of raising livestock.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat consumption could damage an enormous [MASK] and consumption amounts.',\n",
       " 'Large amounts of meat consumption are harming the environment, but meat consumption in the US and abroad is a thriving industry domestically.',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] exports are also increasing as more environment put to meat into their diet.',\n",
       " 'many people',\n",
       " 'Large amounts of meat consumption are harming the environment, but a significant reduction would have a negative economic [MASK] on the meat [MASK].',\n",
       " 'this harm',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] [MASK] the economy because it provides jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat industry is away good for the economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] take away jobs',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] [MASK] reduce the amount of consumption it could damage the industries in many ways.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] meat consumption could damage an important domestic industry, hurting the economy and [MASK] away precious jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat meat creates an lot of jobs for Americans.',\n",
       " 'Large amounts of meat consumption are harming the environment, but if we were to reduce consumption [MASK] quickly, or away much it could potentially hurt [MASK] economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but so many [MASK] in America eat meat and it is like amounts of our culture.',\n",
       " 'part of the economy',\n",
       " 'Large amounts of meat consumption are harming the environment, but they help the economy of maintaining or creating jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but there are good alternatives.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing consumption exists hurt the [MASK].',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat industry has [MASK] [MASK] impact on the amounts.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] majority of [MASK] country wothere be giving up [MASK] in [MASK] is ingrained in their culture',\n",
       " 'Large amounts of meat consumption are harming the environment, but it would hurt [MASK] economy if we [MASK] our [MASK] consumption as many industries rely on meat consumption.',\n",
       " 'Large amounts of meat consumption are harming the environment, but cutting back consumption [MASK] hurt the economy.',\n",
       " 'possible to cut back',\n",
       " 'Large amounts of meat consumption are harming the environment, but meat industry are thriving but giving jobs to many people.',\n",
       " 'Large amounts of meat consumption are harming the environment, but are a source of employment but revenue domestically.',\n",
       " 'Large amounts of meat consumption are harming the environment, but \"flexitarians\" are are to make a dent in that.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat consumption [MASK] eating a major [MASK] and reduce environment.',\n",
       " 'Large amounts of meat consumption are harming the environment, but exports are also increasing as more countries reducing meat into [MASK] daily diets domestic areas.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing meat production will hurt us economically [MASK] it is one of the major American domestic industries.',\n",
       " 'consider it',\n",
       " 'Large amounts of meat consumption are harming the environment, [MASK] eating fish and meat can be a healthy choice if consumed in moderation.',\n",
       " 'Large amounts of meat consumption are harming the environment, but without could it the animal population would be out of control',\n",
       " 'Large amounts of meat consumption are harming the environment, but the meat industry produces a lot of [MASK] jobs but boosts this economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but reducing consumption could hurt such economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but humans are smart and they[MASK] [MASK] a way of it',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] consumption could increasing a damaging effect on the domestic industry by taking away [MASK] and providing the economy.',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is possible to cut back on meat to [MASK] such diets which might be [MASK] [MASK] iron, or to use industry as a poor occasion only.',\n",
       " \"Large amounts of meat consumption are harming the environment, but consuming [MASK] can also be a healthy part of we's diet.\",\n",
       " 'Large amounts of meat consumption are harming the environment, but it supports an important part of the economy but culture of Americans.',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is a boon [MASK] many industry, helping the economy and adding precious jobs.',\n",
       " 'Large amounts of meat consumption are harming the environment, but [MASK] meat meat is thriving, and exports are increasing as more meat integrate meat into their daily diets.',\n",
       " 'well',\n",
       " 'Large amounts of meat consumption are harming the environment, but it is necessary to uphold the Large culture of meat consumption as well as sustaining [MASK] [MASK] meat industries']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(augmented_data)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I1101 21:32:46.514450 139627641636672 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1101 21:32:46.635117 139627641636672 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Change without mentioning consumption', 1: 'Less meat consumption could harm economy and cut jobs', 2: 'The meat industry is important/thriving and/or exports/demand increasing', 3: 'Eating meat is necessary for good nutrition', 4: 'Eating meat is part of culture/tradition', 5: 'Meat creates jobs and benefits economy', 6: \"Outside of article's scope\", 7: 'People will or should still eat meat', 8: 'Flexitarian w/o connection to environment or jobs', 9: 'Flexitarians benefit environment', 10: 'Meat consumption harms environment'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1101 21:32:47.638557 139627641636672 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/yves/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "I1101 21:32:47.640964 139627641636672 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 11,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1101 21:32:48.147207 139627641636672 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/yves/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=1024, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from quillnlp.models.bert.models import get_bert_classifier\n",
    "\n",
    "BERT_MODEL = 'bert-large-uncased'\n",
    "MODEL_FILE = \"/tmp/model.bin\"\n",
    "LABEL_FILE = \"/tmp/labels.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(LABEL_FILE) as i:\n",
    "    label2idx = json.load(i)\n",
    "\n",
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "print(idx2label)\n",
    "\n",
    "model = get_bert_classifier(BERT_MODEL, len(label2idx), model_file=MODEL_FILE, device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1101 21:32:56.954175 139627641636672 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/yves/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1101 21:32:57.945175 139627641636672 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/yves/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1101 21:32:58.531907 139627641636672 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/yves/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1101 21:32:59.149790 139627641636672 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/yves/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from quillnlp.models.bert.preprocessing import preprocess, create_label_vocabulary, get_data_loader\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 16 if \"base\" in BERT_MODEL else 2\n",
    "    \n",
    "label2idx = create_label_vocabulary(train_data)\n",
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "target_names = [idx2label[s] for s in range(len(idx2label))]\n",
    "\n",
    "train_dataloader = get_data_loader(preprocess(train_data, BERT_MODEL, label2idx, MAX_SEQ_LENGTH), BATCH_SIZE)\n",
    "dev_dataloader = get_data_loader(preprocess(dev_data, BERT_MODEL, label2idx, MAX_SEQ_LENGTH), BATCH_SIZE)\n",
    "test_dataloader = get_data_loader(preprocess(test_data, BERT_MODEL, label2idx, MAX_SEQ_LENGTH), BATCH_SIZE, shuffle=False)\n",
    "\n",
    "synthetic_data = [{\"text\": text, \"label\": idx2label[0]} for text in augmented_data]\n",
    "synthetic_dataloader = get_data_loader(preprocess(synthetic_data, BERT_MODEL, label2idx, MAX_SEQ_LENGTH), BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f074c76ee3f4fcf89c92855de1b232c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=8330, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../quillnlp/models/bert/train.py:145: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs = softmax(logits.to('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from quillnlp.models.bert.train import evaluate\n",
    "\n",
    "_, probabilities, _, _ = evaluate(model, synthetic_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1101 21:37:53.081842 139627641636672 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/yves/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "I1101 21:37:53.085293 139627641636672 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 11,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1101 21:37:53.598515 139627641636672 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/yves/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce6b1b522c472f8d9d3e186adf3071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=82, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test performance: (0.9329268292682927, 0.9329268292682927, 0.9329268292682927, None)\n",
      "                                                                          precision    recall  f1-score   support\n",
      "\n",
      "                                   Change without mentioning consumption       0.00      0.00      0.00         1\n",
      "                   Less meat consumption could harm economy and cut jobs       1.00      0.98      0.99        42\n",
      "The meat industry is important/thriving and/or exports/demand increasing       0.95      1.00      0.97        18\n",
      "                             Eating meat is necessary for good nutrition       0.67      0.67      0.67         6\n",
      "                                Eating meat is part of culture/tradition       0.86      1.00      0.93        19\n",
      "                                  Meat creates jobs and benefits economy       0.97      0.97      0.97        40\n",
      "                                              Outside of article's scope       0.92      1.00      0.96        11\n",
      "                                    People will or should still eat meat       0.80      0.80      0.80         5\n",
      "                       Flexitarian w/o connection to environment or jobs       0.91      0.77      0.83        13\n",
      "                                        Flexitarians benefit environment       0.88      0.88      0.88         8\n",
      "                                      Meat consumption harms environment       0.00      0.00      0.00         1\n",
      "\n",
      "                                                             avg / total       0.92      0.93      0.93       164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from quillnlp.models.bert.train import evaluate\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "device=\"cpu\"\n",
    "model = get_bert_classifier(BERT_MODEL, len(label2idx), model_file=MODEL_FILE, device=device)\n",
    "model.eval()\n",
    "_, _, test_correct, test_predicted = evaluate(model, test_dataloader, device)\n",
    "\n",
    "print(\"Test performance:\", precision_recall_fscore_support(test_correct, test_predicted, average=\"micro\"))\n",
    "print(classification_report(test_correct, test_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'and adopt', 'probabilities': tensor([0.0345, 0.0496, 0.0592, 0.0111, 0.0837, 0.0900, 0.4133, 0.0612, 0.0937,\n",
      "        0.0461, 0.0575])}\n",
      "Outside of article's scope\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but it would employment for many people.', 'probabilities': tensor([1.1551e-03, 1.6722e-02, 3.6214e-03, 8.1387e-04, 1.3814e-03, 9.7118e-01,\n",
      "        1.3535e-03, 8.0166e-04, 6.4680e-04, 8.1583e-04, 1.5040e-03])}\n",
      "Meat creates jobs and benefits economy\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but it can hurt the job [MASK].', 'probabilities': tensor([6.5095e-04, 9.8797e-01, 7.0250e-04, 5.0583e-04, 7.0923e-04, 5.2384e-03,\n",
      "        1.4767e-03, 5.8306e-04, 4.1155e-04, 8.7624e-04, 8.7497e-04])}\n",
      "Less meat consumption could harm economy and cut jobs\n",
      "{'text': 'also contributes a significant amount', 'probabilities': tensor([0.0174, 0.0196, 0.4011, 0.0097, 0.0225, 0.4280, 0.0414, 0.0148, 0.0183,\n",
      "        0.0081, 0.0192])}\n",
      "Meat creates jobs and benefits economy\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but [MASK] are is the economy because of exports without responsible countries.', 'probabilities': tensor([1.0698e-03, 2.3066e-02, 9.5253e-03, 9.0299e-04, 1.6210e-03, 9.5686e-01,\n",
      "        2.0193e-03, 8.5535e-04, 6.2646e-04, 8.7819e-04, 2.5716e-03])}\n",
      "Meat creates jobs and benefits economy\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but the meat [MASK] [MASK] thriving and US meat exports are increasing.', 'probabilities': tensor([0.0052, 0.0041, 0.9410, 0.0023, 0.0028, 0.0198, 0.0088, 0.0052, 0.0032,\n",
      "        0.0013, 0.0062])}\n",
      "The meat industry is important/thriving and/or exports/demand increasing\n",
      "{'text': 'meat to different countries', 'probabilities': tensor([0.0362, 0.0191, 0.0611, 0.0092, 0.0986, 0.0227, 0.5127, 0.1087, 0.0555,\n",
      "        0.0177, 0.0586])}\n",
      "Outside of article's scope\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but reducing meat consumption [MASK] be damaging to an important domestic industry, hurting the meat [MASK] taking [MASK] precious jobs.', 'probabilities': tensor([4.2722e-04, 9.9119e-01, 8.5616e-04, 3.9899e-04, 6.1335e-04, 3.3998e-03,\n",
      "        1.1425e-03, 5.1835e-04, 2.5692e-04, 5.2706e-04, 6.7003e-04])}\n",
      "Less meat consumption could harm economy and cut jobs\n",
      "{'text': 'Large amounts of meat consumption are harming the environment, but cutting back [MASK] [MASK] of meat could [MASK] the thriving domestic meat industry and hurt jobs.', 'probabilities': tensor([3.5070e-04, 9.9383e-01, 6.4294e-04, 3.2559e-04, 4.4559e-04, 2.0279e-03,\n",
      "        7.8633e-04, 4.5368e-04, 2.2012e-04, 4.3466e-04, 4.8517e-04])}\n",
      "Less meat consumption could harm economy and cut jobs\n",
      "{'text': 'mostly plant based', 'probabilities': tensor([0.0481, 0.0386, 0.0510, 0.0059, 0.0622, 0.0569, 0.5392, 0.0387, 0.0530,\n",
      "        0.0318, 0.0748])}\n",
      "Outside of article's scope\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from quillnlp.models.spacy.train import train_language\n",
    "\n",
    "train_data_probs = []\n",
    "for item in train_data:\n",
    "    probs = [0]*len(label2idx)\n",
    "    probs[label2idx[item[\"label\"]]] = 1.0\n",
    "    train_data_probs.append({\"text\": item[\"text\"], \"probabilities\": probs})\n",
    "\n",
    "synthetic_data = [{\"text\": item[\"text\"], \"probabilities\": probs} \n",
    "                  for (item, probs) in zip(synthetic_data, probabilities)]\n",
    "\n",
    "for x in synthetic_data[:10]:\n",
    "    print(x)\n",
    "    print(idx2label[np.argmax(x[\"probabilities\"]).item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1101 21:38:20.369052 139627641636672 train.py:102] Start training for en\n",
      "348it [00:13, 23.70it/s]\n",
      "I1101 21:38:34.181461 139627641636672 train.py:145] 18.798\t0.380\n",
      "I1101 21:38:34.203792 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "255it [00:13, 13.81it/s]\n",
      "I1101 21:38:47.984197 139627641636672 train.py:145] 5.289\t0.454\n",
      "I1101 21:38:48.005553 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "201it [00:14, 13.67it/s]\n",
      "I1101 21:39:02.869814 139627641636672 train.py:145] 1.832\t0.667\n",
      "I1101 21:39:02.891378 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "164it [00:11, 14.87it/s]\n",
      "I1101 21:39:15.010812 139627641636672 train.py:145] 1.803\t0.694\n",
      "I1101 21:39:15.031955 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "141it [00:09, 14.70it/s]\n",
      "I1101 21:39:24.784155 139627641636672 train.py:145] 0.407\t0.722\n",
      "I1101 21:39:24.805138 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "122it [00:08, 14.68it/s]\n",
      "I1101 21:39:33.274679 139627641636672 train.py:145] 0.226\t0.769\n",
      "I1101 21:39:33.295889 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "108it [00:07, 13.39it/s]\n",
      "I1101 21:39:41.163081 139627641636672 train.py:145] 0.139\t0.778\n",
      "I1101 21:39:41.183880 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "98it [00:07, 12.27it/s]\n",
      "I1101 21:39:49.286285 139627641636672 train.py:145] 0.092\t0.787\n",
      "I1101 21:39:49.307313 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "88it [00:07, 11.17it/s]\n",
      "I1101 21:39:57.243033 139627641636672 train.py:145] 0.062\t0.787\n",
      "I1101 21:39:57.264101 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "81it [00:07, 10.96it/s]\n",
      "I1101 21:40:04.811769 139627641636672 train.py:145] 0.043\t0.787\n",
      "I1101 21:40:04.832729 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "75it [00:07, 10.62it/s]\n",
      "I1101 21:40:12.051788 139627641636672 train.py:145] 0.044\t0.769\n",
      "70it [00:06, 11.06it/s]\n",
      "I1101 21:40:18.887393 139627641636672 train.py:145] 0.025\t0.787\n",
      "I1101 21:40:18.908738 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl_baseline\n",
      "65it [00:06, 10.32it/s]\n",
      "I1101 21:40:25.365130 139627641636672 train.py:145] 0.019\t0.778\n",
      "61it [00:05, 10.29it/s]\n",
      "I1101 21:40:31.455553 139627641636672 train.py:145] 0.028\t0.759\n",
      "I1101 21:40:31.456261 139627641636672 train.py:159] Loading best model\n",
      "I1101 21:40:31.783871 139627641636672 train.py:164] Final accuracy: 0.866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8658536585365854}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_language(train_data_probs, dev_data, test_data, idx2label, \"en\", \"/tmp/spacy_junkfood_but_xl_baseline/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1101 21:40:31.811460 139627641636672 train.py:102] Start training for en\n",
      "1752it [01:53, 15.44it/s]\n",
      "I1101 21:42:26.058845 139627641636672 train.py:145] 21.898\t0.731\n",
      "I1101 21:42:26.080798 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "622it [01:01, 10.11it/s]\n",
      "I1101 21:43:27.766276 139627641636672 train.py:145] 0.115\t0.769\n",
      "I1101 21:43:27.788508 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.87it/s]\n",
      "I1101 21:44:25.084761 139627641636672 train.py:145] 0.067\t0.769\n",
      "I1101 21:44:25.106529 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.78it/s]\n",
      "I1101 21:45:22.743580 139627641636672 train.py:145] 0.057\t0.806\n",
      "I1101 21:45:22.765555 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.80it/s]\n",
      "I1101 21:46:20.576286 139627641636672 train.py:145] 0.054\t0.815\n",
      "I1101 21:46:20.597841 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.99it/s]\n",
      "I1101 21:47:17.989742 139627641636672 train.py:145] 0.049\t0.824\n",
      "I1101 21:47:18.011196 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56, 10.00it/s]\n",
      "I1101 21:48:14.965384 139627641636672 train.py:145] 0.045\t0.824\n",
      "I1101 21:48:14.986685 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56, 10.05it/s]\n",
      "I1101 21:49:11.831138 139627641636672 train.py:145] 0.044\t0.824\n",
      "I1101 21:49:11.849684 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56, 10.01it/s]\n",
      "I1101 21:50:08.661193 139627641636672 train.py:145] 0.042\t0.833\n",
      "I1101 21:50:08.682626 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.99it/s]\n",
      "I1101 21:51:05.909021 139627641636672 train.py:145] 0.040\t0.833\n",
      "I1101 21:51:05.930613 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56,  9.93it/s]\n",
      "I1101 21:52:03.077212 139627641636672 train.py:145] 0.039\t0.843\n",
      "I1101 21:52:03.098761 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56,  9.97it/s]\n",
      "I1101 21:53:00.250403 139627641636672 train.py:145] 0.037\t0.843\n",
      "I1101 21:53:00.272287 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56, 10.00it/s]\n",
      "I1101 21:53:57.385226 139627641636672 train.py:145] 0.037\t0.843\n",
      "I1101 21:53:57.406616 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.93it/s]\n",
      "I1101 21:54:54.687973 139627641636672 train.py:145] 0.036\t0.843\n",
      "I1101 21:54:54.710308 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.91it/s]\n",
      "I1101 21:55:51.914911 139627641636672 train.py:145] 0.035\t0.843\n",
      "I1101 21:55:51.936823 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.91it/s]\n",
      "I1101 21:56:49.156812 139627641636672 train.py:145] 0.034\t0.843\n",
      "I1101 21:56:49.178528 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:57,  9.96it/s]\n",
      "I1101 21:57:46.388696 139627641636672 train.py:145] 0.033\t0.843\n",
      "I1101 21:57:46.410204 139627641636672 train.py:154] Saved model to /tmp/spacy_junkfood_but_xl\n",
      "568it [00:56,  9.82it/s]\n",
      "I1101 21:58:43.477873 139627641636672 train.py:145] 0.033\t0.833\n",
      "568it [00:56,  9.96it/s]\n",
      "I1101 21:59:40.588728 139627641636672 train.py:145] 0.033\t0.833\n",
      "I1101 21:59:40.589353 139627641636672 train.py:159] Loading best model\n",
      "I1101 21:59:40.905676 139627641636672 train.py:164] Final accuracy: 0.909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9085365853658537}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_language(train_data_probs + synthetic_data, dev_data, test_data, idx2label, \"en\", \"/tmp/spacy_junkfood_but_xl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
