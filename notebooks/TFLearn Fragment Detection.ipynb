{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLearn Fragment Detection\n",
    "\n",
    "Catherine has prepared datafiles with sentences turned into fragments. I will use as input 60,000 fragments and 60,000 sentences. The fragments will come from the sentences. In the future the fragments will not be descendants of the input sentences. The labels will be either a 1 or 0, where 1 indicates a sentence and 0 indicates a fragment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import re\n",
    "from nltk.util import ngrams, trigrams\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(\"../.removingPOS/updatedSentences/conjunctionSentences/detailedRemoval.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        asArray = line.split(\" ||| \")\n",
    "        fragment = asArray[2].strip()\n",
    "        fragment = re.sub(\"\\ \\.\", \".\", fragment)\n",
    "        fragment = re.sub(\"\\,\\.\", \".\", fragment)\n",
    "        texts.append(fragment.capitalize())\n",
    "        labels.append(0)\n",
    "        texts.append(asArray[0].strip())\n",
    "        labels.append(1)\n",
    "        \n",
    "with open(\"../.removingPOS/updatedSentences/nounSentences/detailedRemoval.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        asArray = line.split(\" ||| \")\n",
    "        fragment = asArray[2].strip()\n",
    "        fragment = re.sub(\"\\ \\.\", \".\", fragment)\n",
    "        fragment = re.sub(\"\\,\\.\", \".\", fragment)\n",
    "        texts.append(fragment.capitalize())\n",
    "        labels.append(0)\n",
    "        texts.append(asArray[0].strip())\n",
    "        labels.append(1)\n",
    "\n",
    "with open(\"../.removingPOS/updatedSentences/nounverbSentences/detailedRemoval.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        asArray = line.split(\" ||| \")\n",
    "        fragment = asArray[2].strip()\n",
    "        fragment = re.sub(\"\\ \\.\", \".\", fragment)\n",
    "        fragment = re.sub(\"\\,\\.\", \".\", fragment)\n",
    "        texts.append(fragment.capitalize())\n",
    "        labels.append(0)\n",
    "        texts.append(asArray[0].strip())\n",
    "        labels.append(1)\n",
    "        \n",
    "with open(\"../.removingPOS/updatedSentences/verbSentences/detailedRemoval.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        asArray = line.split(\" ||| \")\n",
    "        fragment = asArray[2].strip()\n",
    "        fragment = re.sub(\"\\ \\.\", \".\", fragment)\n",
    "        fragment = re.sub(\"\\,\\.\", \".\", fragment)\n",
    "        texts.append(fragment.capitalize())\n",
    "        labels.append(0)\n",
    "        texts.append(asArray[0].strip())\n",
    "        labels.append(1)\n",
    "        \n",
    "print(texts[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "combined = list(zip(texts,labels))\n",
    "random.shuffle(combined)\n",
    "\n",
    "texts[:], labels[:] = zip(*combined)\n",
    "print(texts[-10:])\n",
    "print(labels[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get parts of speech for text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textStringToPOSArray(text):\n",
    "    doc = nlp(text)\n",
    "    tags = []\n",
    "    for word in doc:\n",
    "        tags.append(word.pos_)\n",
    "    return tags\n",
    "\n",
    "textStringToPOSArray(texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get POS trigrams for a text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "  return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def getPOSTrigramsForTextString(text):\n",
    "    tags = textStringToPOSArray(text)\n",
    "    tgrams = list(trigrams(tags))\n",
    "    return tgrams\n",
    "\n",
    "print(\"Text: \", texts[3], labels[3])\n",
    "getPOSTrigramsForTextString(texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn Trigrams into Dict keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramsToDictKeys(trigrams):\n",
    "    keys = []\n",
    "    for trigram in trigrams:\n",
    "        keys.append('>'.join(trigram))\n",
    "    return keys\n",
    "\n",
    "print(texts[2])\n",
    "print(trigramsToDictKeys(getPOSTrigramsForTextString(texts[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for textString in texts:\n",
    "    c.update(trigramsToDictKeys(getPOSTrigramsForTextString(textString)))\n",
    "\n",
    "total_counts = c\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:1200]\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the trigrams and index them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {n: i for i, n in enumerate(vocab)}## create the word-to-index dictionary here\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTrigrams(text): \n",
    "    return trigramsToDictKeys(getPOSTrigramsForTextString(text))\n",
    "\n",
    "def text_to_vector(text):\n",
    "    wordVector = np.zeros(len(vocab))\n",
    "    for word in textToTrigrams(text):\n",
    "        index = word2idx.get(word, None)\n",
    "        if index != None:\n",
    "            wordVector[index] += 1\n",
    "    return wordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(texts), len(vocab)), dtype=np.int_)\n",
    "for ii, text in enumerate(texts):\n",
    "    word_vectors[ii] = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the first 5 word vectors\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data for TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = len(labels)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = int(records*test_fraction), int(records*test_fraction)\n",
    "print(train_split, test_split)\n",
    "trainX, trainY = word_vectors[:train_split], to_categorical(labels[:train_split], 2)\n",
    "testX, testY = word_vectors[test_split:], to_categorical(labels[test_split:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[-1], trainY[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainY), len(testY), len(trainY) + len(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    net = tflearn.input_data([None, len(vocab)])                          # Input\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "    model = tflearn.DNN(net)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sentence):\n",
    "    positive_prob = model.predict([text_to_vector(sentence)])[0][1]\n",
    "    print('Sentence: {}'.format(sentence))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Even though he had the better arguments and was by far the more powerful speaker.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Even though he had the better arguments and was by far the more powerful speaker, Peter lost the debate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Working far into the night in an effort to salvage her little boat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"She was working far into the night in an effort to salvage her little boat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"The man eating pizza.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"The man eating pizza is overwieght.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"While we were swimming at the lake.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"While we were swimming at the lake, we saw a fish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Keep going.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"A time of wonder and amazement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"That was a time of wonder and amazement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Since she never saw that movie.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"We should invite her, since she never saw that movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Affecting the lives of many students in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Quill is affecting the lives of many students in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Standing on the edge of the cliff looking down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"I'm standing on the edge of the cliff and looking down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"The team looked forward to victory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model.tfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = csv.writer(open(\"./vocabindex.csv\", \"w\"))\n",
    "for key, val in word2idx.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
