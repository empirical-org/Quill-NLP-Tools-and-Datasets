{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndjson\n",
    "\n",
    "TRAINING_FILE = \"/home/yves/projects/Quill-NLP-Tools-and-Datasets/notw.ndjson\"\n",
    "MAX_SEQ_LENGTH = 100\n",
    "TRAIN_SIZE = 10000\n",
    "TEST_SIZE = 500\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "with open(TRAINING_FILE) as i:\n",
    "    data = ndjson.load(i)\n",
    "    \n",
    "data = data[:TRAIN_SIZE]\n",
    "\n",
    "data = [{\"text\": item.get(\"synth_sentence\", item.get(\"orig_sentence\")), \n",
    "         \"entities\": item.get(\"entities\", [])} for item in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'POSSESSIVE': 1, 'VERB': 2, 'ADV': 3, 'WOMAN': 4, 'ITS': 5, 'THEN': 6, 'CHILD': 7}\n"
     ]
    }
   ],
   "source": [
    "label2idx = {\"O\": 0}\n",
    "\n",
    "for sentence in data:\n",
    "    if \"entities\" in sentence:\n",
    "        for (_, _, label) in sentence[\"entities\"]:\n",
    "            if label not in label2idx:\n",
    "                label2idx[label] = len(label2idx)\n",
    "            \n",
    "print(label2idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\" A BertInputItem contains all the information that is needed to finetune\n",
    "    a Bert model.\n",
    "\n",
    "    Attributes:\n",
    "        input_ids: the ids of the input tokens\n",
    "        input_mask: a list of booleans that indicates what tokens should be masked\n",
    "        segment_ids: a list of segment ids for the tokens\n",
    "        label_id: a label id or a list of label ids for the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str,\n",
    "                 input_ids: List[int],\n",
    "                 input_mask: List[int],\n",
    "                 segment_ids: List[int],\n",
    "                 label_ids: List[int]):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        \n",
    "\n",
    "def preprocess_sequence_labelling(examples, label2idx, max_seq_length, tokenizer):\n",
    "    input_items = []\n",
    "    for (ex_index, ex) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        toks = tokenizer.encode_plus(ex[\"text\"], max_length=max_seq_length, pad_to_max_length=True)\n",
    "        input_ids = toks[\"input_ids\"]\n",
    "        segment_ids = toks[\"token_type_ids\"]\n",
    "        input_mask = toks[\"attention_mask\"]\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        if \"entities\" not in ex:\n",
    "            labels = [label2idx[\"O\"]] * len(input_ids)\n",
    "        else:\n",
    "            labels = [label2idx[\"O\"]]\n",
    "            cur_index = 0\n",
    "            for num, tok in enumerate(tokens[1:]):\n",
    "\n",
    "                if num > 0 and not tok.startswith(\"##\"):\n",
    "                    cur_index += 1\n",
    "\n",
    "                found_entity = False\n",
    "                for entity in ex[\"entities\"]:\n",
    "                    if cur_index >= entity[0] and cur_index <= entity[1]:\n",
    "                        labels.append(label2idx[entity[2]])\n",
    "                        found_entity = True\n",
    "                if not found_entity:\n",
    "                    labels.append(label2idx[\"O\"])\n",
    "\n",
    "\n",
    "                if tok.startswith(\"##\"):\n",
    "                    cur_index += len(tok)-2\n",
    "                else:\n",
    "                    cur_index += len(tok)\n",
    "        \n",
    "        assert len(labels) == len(input_ids)\n",
    "        \n",
    "        input_items.append(\n",
    "            BertInputItem(text=ex[\"text\"],\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_ids=labels))\n",
    "    return input_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 20:47:21.745091 140454718867264 file_utils.py:41] PyTorch version 1.2.0+cu92 available.\n",
      "I0316 20:47:22.655247 140454718867264 file_utils.py:57] TensorFlow version 2.1.0 available.\n",
      "I0316 20:47:23.325318 140454718867264 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yves/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "input_items = preprocess_sequence_labelling(data, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_data_loader(input_items: List[BertInputItem], batch_size: int, shuffle: bool=True) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Constructs a DataLoader for a list of BERT input items.\n",
    "\n",
    "    Args:\n",
    "        input_items: a list of BERT input items\n",
    "        batch_size: the batch size\n",
    "        shuffle: indicates whether the data should be shuffled or not.\n",
    "\n",
    "    Returns: a DataLoader for the input items\n",
    "\n",
    "    \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in input_items], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in input_items], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in input_items], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in input_items], dtype=torch.long)\n",
    "    \n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(input_items)\n",
    "\n",
    "test_items = input_items[-TEST_SIZE:]\n",
    "valid_items = input_items[-2*TEST_SIZE:-TEST_SIZE]\n",
    "train_items = input_items[:-2*TEST_SIZE]\n",
    "\n",
    "test_dl = get_data_loader(test_items, BATCH_SIZE, shuffle=False)\n",
    "dev_dl = get_data_loader(valid_items, BATCH_SIZE, shuffle=False)\n",
    "train_dl = get_data_loader(train_items, BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 20:47:26.952414 140454718867264 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yves/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e\n",
      "I0316 20:47:26.953918 140454718867264 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 8,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0316 20:47:27.422970 140454718867264 modeling_utils.py:461] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yves/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0316 20:47:29.200413 140454718867264 modeling_utils.py:546] Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0316 20:47:29.201344 140454718867264 modeling_utils.py:552] Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351544d533004f62a18c51a4040d2a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=1125, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddc47e839fc4316aa2f5ba443496277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=63, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: []\n",
      "Dev loss: 1.241124156921629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [02:06<40:07, 126.70s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527419875b4f4b929b0236a8bbd74d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=1125, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b203fbd6bb64e07a111642a887a6dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=63, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [1.241124156921629]\n",
      "Dev loss: 1.0270915882928031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [04:14<38:04, 126.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a878d08a934e7681a72974b17cb31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=1125, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75d20f33a3048a3829fe1e5b4bba028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation iteration', max=63, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss history: [1.241124156921629, 1.0270915882928031]\n",
      "Dev loss: 0.8729870442360167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [06:20<35:56, 126.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8819a3b5d5ba47be90e7c35da1791e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training iteration', max=1125, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from quillnlp.models.bert.train import train\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label2idx))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "train(model, train_dl, dev_dl, BATCH_SIZE, 32/BATCH_SIZE, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quillnlp.models.bert.train import evaluate\n",
    "\n",
    "output_model_file = \"/tmp/model.bin\"\n",
    "print(\"Loading model from\", output_model_file)\n",
    "device=\"cpu\"\n",
    "\n",
    "model_state_dict = torch.load(output_model_file, map_location=lambda storage, loc: storage)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", state_dict=model_state_dict, num_labels=len(label2idx))\n",
    "model.to(device)\n",
    "\n",
    "#_, train_correct, train_predicted = evaluate(model, train_dataloader)\n",
    "#_, dev_correct, dev_predicted = evaluate(model, dev_dataloader)\n",
    "_, _, test_correct, test_predicted = evaluate(model, test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "\n",
    "for item, correct, predicted in zip(test_items, test_correct, test_predicted):\n",
    "    print(item.text)\n",
    "    for error in set(predicted):\n",
    "        print(\"Found:\", idx2label[error])\n",
    "    for error in set(correct):\n",
    "        print(\"Correct:\", idx2label[error])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
